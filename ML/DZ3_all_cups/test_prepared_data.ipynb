{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-21T08:25:35.987472Z",
     "start_time": "2024-04-21T08:25:35.553923Z"
    }
   },
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_test_data.pickle', 'rb') as file:\n",
    "    train_test_data = pickle.load(file)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:25:36.003473Z",
     "start_time": "2024-04-21T08:25:35.988472Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "76e051a443fc02e3",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:25:36.607217Z",
     "start_time": "2024-04-21T08:25:36.004472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "VAL_DATA = pd.read_csv(\"test.csv\")\n",
    "# VAL_DATA = test_df['title'].values"
   ],
   "id": "295341d2982e5f4e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:25:38.493932Z",
     "start_time": "2024-04-21T08:25:36.608219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from my_librery import *"
   ],
   "id": "50ef58231c522936",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:25:38.508931Z",
     "start_time": "2024-04-21T08:25:38.494932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "TOKEN_PATTERN = \"[а-яё]+\"\n",
    "STOP_WORDS = nltk.corpus.stopwords.words('russian') + nltk.corpus.stopwords.words('english')"
   ],
   "id": "cfb8d95a44fff476",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T20:24:57.873566Z",
     "start_time": "2024-04-17T20:24:14.524891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import RobustScaler, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features',\n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=1, ngram_range=(0, 3)),\n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=4, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('norm', Normalizer()),\n",
    "    ('scale', RobustScaler(with_centering=False)),\n",
    "    ('clf', nb.BernoulliNB())\n",
    "])\n",
    "fit_predict(pipeline, *train_test_data)"
   ],
   "id": "9c59bee23c9ab84c",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:10:30.443595Z",
     "start_time": "2024-04-18T10:08:47.499450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features', \n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)), \n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'features__title__max_df': [0.05, 0.04, 0.06, 0.08, 0.1],\n",
    "    'features__title__min_df': [3,  2, 4, 5],\n",
    "    'features__title__ngram_range': [(0, 3), (0, 2), (0, 4), (0, 5)],\n",
    "\n",
    "    # Добавьте другие параметры, которые хотите варьировать\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(train_test_data[0], train_test_data[1])\n",
    "\n",
    "# Лучшие параметры и оценщик\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "f6e792bf26847271",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:25:50.304482Z",
     "start_time": "2024-04-21T08:25:49.745234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_unicode(df, column):\n",
    "    df[column] = df[column].apply(lambda x: ''.join(i for i in x if ord(i)<128))\n",
    "    return df\n",
    "\n",
    "train_df_new = remove_unicode(train_test_data[0], 'title')\n",
    "val_df_new = remove_unicode(train_test_data[2], 'title')"
   ],
   "id": "cc0b3e887ba8189",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:26:43.071930Z",
     "start_time": "2024-04-21T08:26:43.068928Z"
    }
   },
   "cell_type": "code",
   "source": "new_train_test_data = [train_df_new, train_test_data[1], val_df_new, train_test_data[3]]",
   "id": "80dc9dc9dd450f7d",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:12:10.166279Z",
     "start_time": "2024-04-18T10:10:30.444598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features',\n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'features__title__max_df': [0.05, 0.04, 0.06, 0.08, 0.1],\n",
    "    'features__title__min_df': [3,  2, 4, 5],\n",
    "    'features__title__ngram_range': [(0, 3), (0, 2), (0, 4), (0, 5)],\n",
    "\n",
    "    # Добавьте другие параметры, которые хотите варьировать\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(new_train_test_data[0], new_train_test_data[1])\n",
    "\n",
    "# Лучшие параметры и оценщик\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "f3fb8aeee104aed",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:59:50.731850Z",
     "start_time": "2024-04-18T15:59:46.386736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "the_best = Pipeline(steps=[('features',\n",
    "                 ColumnTransformer(transformers=[('url',\n",
    "                                                  TfidfVectorizer(max_df=0.05,\n",
    "                                                                  min_df=3,\n",
    "                                                                  ngram_range=(0,\n",
    "                                                                               3)),\n",
    "                                                  'url'),\n",
    "                                                 ('title',\n",
    "                                                  TfidfVectorizer(max_df=0.06,\n",
    "                                                                  min_df=2,\n",
    "                                                                  ngram_range=(0,\n",
    "                                                                               2)),\n",
    "                                                  'title')])),\n",
    "                ('clf', MultinomialNB(alpha=1))])\n",
    "fit_predict(the_best, *train_test_data)\n"
   ],
   "id": "72cf767fc25a8da1",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:51:13.156467Z",
     "start_time": "2024-04-18T15:51:13.109955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "the_best = Pipeline(steps=[('features',\n",
    "                 ColumnTransformer(transformers=[('url',\n",
    "                                                  TfidfVectorizer(max_df=0.05,\n",
    "                                                                  min_df=3,\n",
    "                                                                  ngram_range=(0,\n",
    "                                                                               3)),\n",
    "                                                  'url'),\n",
    "                                                 ('title',\n",
    "                                                  TfidfVectorizer(max_df=0.06,\n",
    "                                                                  min_df=2,\n",
    "                                                                  ngram_range=(0,\n",
    "                                                                               2)),\n",
    "                                                  'title')])),\n",
    "                ('clf', MultinomialNB(alpha=1))])\n",
    "# fit_predict(the_best, *new_train_test_data)\n"
   ],
   "id": "4f6718ecb759fdf8",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:27:44.817835Z",
     "start_time": "2024-04-21T08:27:44.794324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "all_data = pd.concat([new_train_test_data[0], new_train_test_data[2]])\n",
    "all_y = np.concatenate([new_train_test_data[1], new_train_test_data[3]])\n",
    "\n",
    "# Предположим, что df - это ваш DataFrame, а labels - это ваш массив numpy с метками классов\n",
    "labels_df = pd.DataFrame(all_y, columns=['label'])\n",
    "all_data.reset_index(drop=True, inplace=True)\n",
    "# Конкатенируем df и labels_df\n",
    "all_data_frame = pd.concat([all_data, labels_df],axis=1)\n",
    "all_data_frame.shape"
   ],
   "id": "ee187e2461fd54f9",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T08:28:51.991942Z",
     "start_time": "2024-04-21T08:28:51.927899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_total_length(text):\n",
    "    return len(text.split()) > 3\n",
    "\n",
    "# применяем функцию к колонке 'title' и оставляем только те строки, где общая длина всех слов больше или равна трем\n",
    "df = all_data_frame[all_data_frame['title'].apply(check_total_length)]\n",
    "\n",
    "df.shape"
   ],
   "id": "224accbb0e92dfbf",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T18:07:28.293961Z",
     "start_time": "2024-04-18T18:07:27.906735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Предположим, что df - это ваш DataFrame, а train_values - это ваши метки классов\n",
    "\n",
    "\n",
    "# Отфильтровываем строки, принадлежащие классу 1\n",
    "class_1_titles = all_data_frame[all_data_frame['label'] == 0]['title']\n",
    "\n",
    "# Разбиваем строки на слова и подсчитываем количество каждого слова\n",
    "word_counts = Counter()\n",
    "for title in class_1_titles:\n",
    "    words = re.findall(r'\\b\\w+\\b', title)\n",
    "    word_counts.update(words)\n",
    "\n",
    "# Выводим наиболее часто встречающиеся слова\n",
    "# print(word_counts.most_common())\n",
    "# Получаем список наиболее часто встречающихся слов\n",
    "most_common_words = word_counts.most_common()\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# Фильтруем список, чтобы оставить только слова, которые встречаются более 170 раз\n",
    "words_over_170 = [word for word, count in most_common_words if count > 170 and not is_number(word)]\n",
    "\n",
    "# Выводим результат\n",
    "print(words_over_170)"
   ],
   "id": "ab66cd4bb0dcea98",
   "execution_count": 91,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T18:23:28.871256Z",
     "start_time": "2024-04-18T18:23:28.853803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Функция для создания нового признака\n",
    "def words_presence_feature(df, words):\n",
    "    return df.apply(lambda x: 1 if any(word in x for word in words) else 0).to_frame()\n",
    "\n",
    "# Создаем список слов, которые вы хотите проверить\n",
    "words_to_check = ['porn', 'sex', 'fuck', 'dick', 'pussy', 'sperm', 'webcam', 'boobs',]\n",
    "\n",
    "# words_to_check = ['porn', 'sex', 'xxx', 'girls', 'big', 'anal', 'naked', 'pussy', 'ass', 'biqle', 'tits', 'fucked', 'daftsex', 'blowjob', 'sexy', 'erotic', 'nude', 'dick', 'porno', 'fuck', 'fucks', 'fucking', 'erotica', 'milf', 'ancensored', 'cum', 'amateur', 'hardcore', 'adult', 'busty', 'lesbian', 'cock', 'homemade', 'xvideos', 'stockings', 'gay', 'chick', 'lesbians', 'boobs', 'masturbation', 'group']\n",
    "for_1_class = FunctionTransformer(words_presence_feature, validate=False, kw_args={'words': words_to_check})\n",
    "for_0_class = FunctionTransformer(words_presence_feature, validate=False, kw_args={'words': words_over_170})\n"
   ],
   "id": "f3d2f2bf502d803c",
   "execution_count": 109,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T18:11:29.959955Z",
     "start_time": "2024-04-18T18:11:29.919910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Добавляем новый признак в пайплайн\n",
    "the_best_1 = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2)), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "# fit_predict(the_best_1, *new_train_test_data)"
   ],
   "id": "a1cdc80b0147fc79",
   "execution_count": 100,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T18:36:12.610352Z",
     "start_time": "2024-04-18T18:36:12.602351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [doc.split() for doc in X]\n",
    "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([abs(np.mean([self.model.wv[w] for w in doc.split() if w in self.model.wv]\n",
    "                                 or [np.zeros(self.vector_size)], axis=0))\n",
    "                         for doc in X])\n",
    "\n"
   ],
   "id": "5566fdfe71824e83",
   "execution_count": 118,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T18:37:33.036484Z",
     "start_time": "2024-04-18T18:37:28.009728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "the_best_1 = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2)), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "fit_predict(the_best_1, *new_train_test_data)"
   ],
   "id": "6ab33aab211ea157",
   "execution_count": 120,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:15:09.679559Z",
     "start_time": "2024-04-18T19:15:09.645016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = nltk.ngrams(text, n)\n",
    "    return [''.join(grams) for grams in n_grams]\n",
    "# sentence = 'лекция протексты'\n",
    "# get_ngrams(sentence, 2)\n",
    "\n",
    "\n",
    "\n",
    "# Создаем функцию для предобработки текста\n",
    "def preprocess_text(text):\n",
    "    # Токенизация\n",
    "    words = word_tokenize(text)\n",
    "    # words = get_ngrams(text, 7)\n",
    "\n",
    "    # Удаление стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Лемматизация\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "# Используем функцию в TfidfVectorizer\n",
    "the_best_1 = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3), tokenizer=preprocess_text), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2), tokenizer=preprocess_text), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "# fit_predict(the_best_1, *new_train_test_data)\n"
   ],
   "id": "b0395edd4b25b7ee",
   "execution_count": 133,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:16:05.077364Z",
     "start_time": "2024-04-18T19:15:12.131080Z"
    }
   },
   "cell_type": "code",
   "source": "the_best_1.fit(all_data, all_y)",
   "id": "4f1571ae25674808",
   "execution_count": 134,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T15:49:52.744172Z",
     "start_time": "2024-04-18T15:49:52.727725Z"
    }
   },
   "cell_type": "code",
   "source": "len(all_data)",
   "id": "b3179836f6afc73b",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:16:08.766905Z",
     "start_time": "2024-04-18T19:16:08.688180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('VAL_DATA.pickle', 'rb') as file:\n",
    "    VAL_DATA = pickle.load(file)"
   ],
   "id": "d362b747b8d2cf52",
   "execution_count": 135,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:17:14.254202Z",
     "start_time": "2024-04-18T19:16:09.446722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = the_best_1.predict(VAL_DATA)\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df[\"label\"] = results\n",
    "\n",
    "test_df[[\"ID\", \"label\"]].to_csv(\"Current_best_merged.csv\", index=False)"
   ],
   "id": "fce3ad0d4fdfb4ce",
   "execution_count": 136,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "c825a413319a23dd",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
