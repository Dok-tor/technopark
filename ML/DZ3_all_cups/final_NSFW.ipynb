{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:08:19.583782Z",
     "start_time": "2024-04-21T09:08:19.271590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "d1738647842913b3",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:08:20.693168Z",
     "start_time": "2024-04-21T09:08:20.097092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "VAL_DATA = pd.read_csv(\"test.csv\")"
   ],
   "id": "f5dc19ebb41a90f4",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T21:23:16.316843Z",
     "start_time": "2024-04-21T21:23:16.302841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from my_librery import *"
   ],
   "id": "9ee86b3514d1a6ed",
   "execution_count": 84,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Здесь много кода, во время эксперементирования я выносил его в отдельный файл и импортировал из ноутбука. Для сдачи собирал в единое целое.",
   "id": "17f7c09f977a7fc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm import tqdm\n",
    "\n",
    "to_translate = 'Привет мир'\n",
    "tran = GoogleTranslator(source='auto', target='en')\n",
    "translated = tran.translate(to_translate)\n",
    "print(translated)\n",
    "\n",
    "from translate import Translator\n",
    "\n",
    "SEED = 42\n",
    "TOKEN_PATTERN = \"[а-яё]+\"\n",
    "STOP_WORDS = nltk.corpus.stopwords.words('russian') + nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def preprocess_news(lst):\n",
    "    new_corpus = []\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "    for text in lst:\n",
    "        words = [w for w in word_tokenize(text) if (w not in STOP_WORDS)]\n",
    "        words = [lem.lemmatize(w) for w in words]\n",
    "        one_string = ' '.join(words)\n",
    "        new_corpus.append(one_string)\n",
    "    return new_corpus\n",
    "\n",
    "\n",
    "tran = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "\n",
    "def clean_text(input_text):\n",
    "    try:\n",
    "        clean_text = tran.translate(input_text)\n",
    "        clean_text = re.sub('<[^<]+?>', '', clean_text)\n",
    "\n",
    "        clean_text = re.sub(r'http\\S+', '', clean_text)\n",
    "\n",
    "        clean_text = emojis_words(clean_text)\n",
    "\n",
    "        clean_text = clean_text.lower()\n",
    "\n",
    "        stop_words = set(stopwords.words('russian'))\n",
    "        tokens = word_tokenize(clean_text, language='russian')\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        clean_text = ' '.join(tokens)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = word_tokenize(clean_text, language='english')\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        clean_text = ' '.join(tokens)\n",
    "\n",
    "        clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "    except:\n",
    "        print(input_text)\n",
    "        clean_text = input_text\n",
    "\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def clean_url(input_text):\n",
    "    clean_text = emojis_words(input_text)\n",
    "\n",
    "    clean_text = clean_text.lower()\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(clean_text, language='english')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_text = ' '.join(tokens)\n",
    "\n",
    "    translator = Translator(to_lang=\"en\")\n",
    "    clean_text = translator.translate(clean_text)\n",
    "\n",
    "    clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "    clean_text = re.sub(r'[^\\w\\s]', ' ', clean_text)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def clean_text_unicode(input_text):\n",
    "    clean_text = re.sub('<[^<]+?>', '', input_text)\n",
    "\n",
    "    clean_text = re.sub(r'http\\S+', '', clean_text)\n",
    "\n",
    "    clean_text = emojis_words(clean_text)\n",
    "\n",
    "    clean_text = clean_text.lower()\n",
    "\n",
    "    clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
    "\n",
    "    clean_text = clean_text_keep_non_ascii(clean_text)\n",
    "    clean_text = contractions.fix(clean_text)\n",
    "\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    tokens = word_tokenize(clean_text, language='russian')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_text = ' '.join(tokens)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(clean_text, language='english')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    clean_text = ' '.join(tokens)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def emojis_words(text):\n",
    "    clean_text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    clean_text = clean_text.replace(\":\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def clean_text_keep_non_ascii(text):\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "    encoded_text = normalized_text.encode('ascii', 'backslashreplace')\n",
    "\n",
    "    decoded_text = encoded_text.decode('utf-8', 'backslashreplace')\n",
    "\n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "def clean_data_list(d_list):\n",
    "    l = []\n",
    "    for string in tqdm(d_list, desc=\"Cleaning data\"):\n",
    "        l.append(clean_text(string))\n",
    "    return l\n",
    "\n",
    "\n",
    "def clean_data_list_unicode(d_list):\n",
    "    l = []\n",
    "    for string in d_list:\n",
    "        l.append(clean_text_unicode(string))\n",
    "    return l\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def clean_url_data_list(d_list):\n",
    "    l = []\n",
    "    for string in tqdm(d_list, desc=\"Cleaning URLs\"):\n",
    "        l.append(clean_url(string))\n",
    "    return l\n",
    "\n",
    "\n",
    "def preprocess_news_after_clear_data(X):\n",
    "    X = clean_data_list(X)\n",
    "    X = preprocess_news(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def fit_predict(model, X_train, Y_train, X_test=None, Y_test=None):\n",
    "    model.fit(X_train, Y_train)\n",
    "    if (X_test is not None and Y_test is not None):\n",
    "        return {\n",
    "            'train': f1_score(Y_train, model.predict(X_train)),\n",
    "            'test': f1_score(Y_test, model.predict(X_test))\n",
    "        }\n",
    "    return {'train': f1_score(Y_train, model.predict(X_train))}\n",
    "\n",
    "\n",
    "def wrong_pridictions(model, X_train, Y_train, X_test=None, Y_test=None):\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    di = {'train': [], 'test': []}\n",
    "    for i, prediction in enumerate(pred_train):\n",
    "        if prediction != Y_train[i]:\n",
    "            di['train'].append((X_train[i], prediction))\n",
    "    for i, prediction in enumerate(pred_test):\n",
    "        if prediction != Y_test[i]:\n",
    "            di['test'].append((X_test[i], prediction))\n",
    "    return di\n",
    "\n"
   ],
   "id": "27bb1e48c6411ee7",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:08:23.339859Z",
     "start_time": "2024-04-21T09:08:23.334859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "TOKEN_PATTERN = \"[а-яё]+\"\n",
    "STOP_WORDS = nltk.corpus.stopwords.words('russian') + nltk.corpus.stopwords.words('english')"
   ],
   "id": "9d9416fd7cafb567",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from word_cloud import plot_word_cloud\n",
    "plot_word_cloud(train_df['title'].values)"
   ],
   "id": "cb557a2a9c770e0a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Здесь были эксперементы с моделями на первичных данных, которые я сохранил в словарь (loaded_dict), к сожалению, потом я изменил формат предобработки данных и этот словарь теперь неопределён. Поэтому эти модели просто будут лежать здесь в доказательство того, что эксперементы были проведены.",
   "id": "21bfb766edf5a23b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "multinom_grid_model = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vectorizer__lowercase': [True, False],\n",
    "    'vectorizer__ngram_range': [(0, 1), (0, 2), (0, 3), (0, 4), (1, 1), (1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4), (3, 3)],\n",
    "    'vectorizer__min_df': [1, 5, 10],\n",
    "    'vectorizer__max_df': [0.5, 0.75, 0.8],\n",
    "    'vectorizer__binary': [True, False],\n",
    "    'clf__alpha': [0.1, 1.0, 10.0],\n",
    "    'clf__fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(multinom_grid_model, param_grid=params, cv=5)\n",
    "grid_search.fit(loaded_dict['clean_data_list'][0], loaded_dict['clean_data_list'][1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "d56c1aee703c6611",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "multinom_grid_model = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    \n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "params = {\n",
    "    'vectorizer__ngram_range': [(0, 1), (0, 2), (0, 3)],\n",
    "    'vectorizer__min_df': [1, 5, 10],\n",
    "    'vectorizer__max_df': [0.5, 0.75, 0.8],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(multinom_grid_model, param_grid=params, scoring=f1_scorer, cv=5)\n",
    "grid_search.fit(loaded_dict['clean_data_list'][0], loaded_dict['clean_data_list'][1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "813fddc9d70bee07",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "multinom_grid_model = Pipeline([('vectorizer',\n",
    "                                 TfidfVectorizer(max_df=0.5, min_df=5, ngram_range=(0, 3))),\n",
    "                                ('clf', MultinomialNB())])\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "params = {\n",
    "    'vectorizer__min_df': [3, 4, 5, 6, 7],\n",
    "    'vectorizer__max_df': [0.3, 0.4, 0.5],\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(multinom_grid_model, param_grid=params, scoring=f1_scorer, cv=5)\n",
    "grid_search.fit(loaded_dict['clean_data_list'][0], loaded_dict['clean_data_list'][1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "75f559f84fe2ab6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "multinom_grid_model = Pipeline([('vectorizer',\n",
    "                                 TfidfVectorizer(max_df=0.5, min_df=5, ngram_range=(0, 3))),\n",
    "                                ('clf', MultinomialNB())])\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "params = {\n",
    "    'vectorizer__min_df': [1, 2, 3],\n",
    "    'vectorizer__max_df': [0.1, 0.2 ,0.3],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(multinom_grid_model, param_grid=params, scoring=f1_scorer, cv=5)\n",
    "grid_search.fit(loaded_dict['clean_data_list'][0], loaded_dict['clean_data_list'][1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "2cfc1f8d7b7b0679",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "multinom_grid_model = Pipeline([('vectorizer',\n",
    "                                 TfidfVectorizer(max_df=0.5, min_df=3, ngram_range=(0, 3))),\n",
    "                                ('clf', MultinomialNB())])\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "params = {\n",
    "    'vectorizer__max_df': [0.01, 0.02, 0.03, 0.05 ,0.1],\n",
    "    'clf__alpha': [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(multinom_grid_model, param_grid=params, scoring=f1_scorer, cv=5)\n",
    "grid_search.fit(loaded_dict['clean_data_list'][0], loaded_dict['clean_data_list'][1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "7a75739cce8b8b80",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "best_pipe = Pipeline(steps=[('vectorizer',\n",
    "                 TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3))),\n",
    "                ('clf', MultinomialNB(alpha=1))])\n",
    "fit_predict(best_pipe, X_train['title'], loaded_dict[\"clean_data_list\"][1], X_test['title'], loaded_dict[\"clean_data_list\"][3])"
   ],
   "id": "d18c6ea6e0723a0a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "multinom_grid_model = Pipeline(steps=[('vectorizer',\n",
    "                                       TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3))),\n",
    "                                      ('clf', MultinomialNB())])\n",
    "\n",
    "params = {\n",
    "    'clf__fit_prior': [True, False],\n",
    "    'clf__force_alpha': [True, False],\n",
    "    'clf__alpha': [0.9, 1.0, 1.1],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(multinom_grid_model, param_grid=params, cv=5)\n",
    "grid_search.fit(loaded_dict['clean_data_list'][0], loaded_dict['clean_data_list'][1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "cf64a665d1bd64f2",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "multinom_grid_model = Pipeline(steps=[('vectorizer',\n",
    "                                       TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3))),\n",
    "                                      ('clf', MultinomialNB(alpha=0.9, force_alpha=True))])\n",
    "\n",
    "params = {\n",
    "    'clf__alpha': [0.6, 0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(multinom_grid_model, param_grid=params, cv=5)\n",
    "grid_search.fit(loaded_dict['clean_data_list'][0], loaded_dict['clean_data_list'][1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "8d20d11dba36b474",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "best_pipe = Pipeline(steps=[('vectorizer',\n",
    "                             TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3))),\n",
    "                            ('clf', MultinomialNB(alpha=0.8, force_alpha=True))])\n",
    "fit_predict(best_pipe, *loaded_dict[\"clean_data_list\"])"
   ],
   "id": "c21171a9659f7eed",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features', \n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)), \n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'features__url__max_df': [0.05, 0.1, 0.2],\n",
    "    'features__url__min_df': [1, 2, 3],\n",
    "    'features__url__ngram_range': [(0, 1), (0, 2), (0, 3)],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(train_test_data[0], train_test_data[1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "1f43ffcfd9af68a6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features', \n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)), \n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'features__url__max_df': [0.05, 0.04, 0.06],\n",
    "    'features__url__ngram_range': [(0, 3),  (0, 4)],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(train_test_data[0], train_test_data[1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "6156a728ea22f296",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from sklearn.preprocessing import RobustScaler, Normalizer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features',\n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=1, ngram_range=(0, 3)),\n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('norm', Normalizer()),\n",
    "    ('scale', RobustScaler(with_centering=False)),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "fit_predict(pipeline, *train_test_data)"
   ],
   "id": "c5bbc20d79cdc564",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.preprocessing import RobustScaler, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features',\n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=1, ngram_range=(0, 3)),\n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('norm', Normalizer()),\n",
    "    ('scale', RobustScaler(with_centering=False)),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "fit_predict(pipeline, *train_test_data)"
   ],
   "id": "c52eb54f010d20d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Здесь я решил посмотреть, какие данные моя модель плохо классифицирует, и понять распределение",
   "id": "da542c0b31e0bd3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def wrong_predictionss(model, X_train, Y_train, X_test=None, Y_test=None):\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    prob_train = model.predict_proba(X_train)\n",
    "    prob_test = model.predict_proba(X_test)\n",
    "    di = {'train': [], 'test': []}\n",
    "    wrong_train_indices = []\n",
    "    for i, prediction in enumerate(pred_train):\n",
    "        if prediction != Y_train[i]:\n",
    "            di['train'].append([X_train.iloc[i], prediction, Y_train[i], prob_train[i]])\n",
    "            wrong_train_indices.append(i)\n",
    "    for i, prediction in enumerate(pred_test):\n",
    "        if prediction != Y_test[i]:\n",
    "            di['test'].append([X_test.iloc[i], prediction, Y_test[i], prob_test[i]])\n",
    "    df_for_refit = pd.concat([X_train.iloc[wrong_train_indices], pd.DataFrame(Y_train).iloc[wrong_train_indices]], axis=1)\n",
    "    print(type(df_for_refit))\n",
    "    return di, df_for_refit\n",
    "\n",
    "wrong, over_train_df = wrong_predictionss(pipeline, *train_test_data)"
   ],
   "id": "95d399125d68ac22",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 31,
   "source": "over_train_df",
   "id": "68d738b099146e3b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Оказалось, что плохо классифицируются самые разные элементы\n",
    "В этот момент мне пришла в голову идея перевести все данные на английский язык"
   ],
   "id": "edfcc6964e486eed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df_train, df_test = train_test_split(train_df, test_size=0.2, random_state=SEED)\n",
    "X_tr = df_train['title'].values\n",
    "Y_tr = df_train['label'].values\n",
    "\n",
    "X_ts = df_test['title'].values\n",
    "Y_ts = df_test['label'].values\n",
    "\n",
    "len(X_tr), len(Y_tr), len(X_ts), len(Y_ts),"
   ],
   "id": "845d3eae2fb1ad14",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d392670c6a9bd02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data_inpl = {'default': [X_tr, Y_tr, X_ts, Y_ts]}\n",
    "\n",
    "def preprocess_data_inplace(func_list=None, title_list=None) -> None:\n",
    "\n",
    "    if func_list is not None:\n",
    "        assert (len(func_list) == len(title_list))\n",
    "        for i in range(len(func_list)):\n",
    "            data_inpl[title_list[i]] = [func_list[i](X_tr), Y_tr, func_list[i](X_ts), Y_ts]\n"
   ],
   "id": "8e5b4ba60b716cf2",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Здесь я запускаю очистку данных, которая включает в себя перевод слов на английский язык, это очень длительный процесс (У меня занял более 2 суток). К сожалению, ультимативного результата добиться не удалось.",
   "id": "2b8f485ec67ccc3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def prepare_data(df, func_url, func_title):\n",
    "    df = pd.DataFrame({\n",
    "        'url': func_url(df['url']),\n",
    "        'title': func_title(df['title']),\n",
    "    })\n",
    "    return df\n",
    "    \n",
    "    \n",
    "X_train = prepare_data(df_train, clean_url_data_list, clean_data_list)\n",
    "X_test = prepare_data(df_test, clean_url_data_list, clean_data_list)\n",
    "train_test_data = [X_train, Y_tr, X_test, Y_ts]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('train_test_data.pickle', 'wb') as file:\n",
    "    pickle.dump(train_test_data, file)\n",
    "\n",
    "print(\"Data is preprocessed\")\n"
   ],
   "id": "55f4d7178fa71673",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 9,
   "source": "VAL_DATA = prepare_data(VAL_DATA, clean_url_data_list, clean_data_list)",
   "id": "b9efdfe8a946ec64",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:08:45.661159Z",
     "start_time": "2024-04-21T09:08:45.624158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_test_data.pickle', 'rb') as file:\n",
    "    train_test_data = pickle.load(file)"
   ],
   "id": "de3548a3882ac13c",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from sklearn.preprocessing import RobustScaler, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features',\n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=1, ngram_range=(0, 3)),\n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=4, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('norm', Normalizer()),\n",
    "    ('scale', RobustScaler(with_centering=False)),\n",
    "    ('clf', nb.BernoulliNB())\n",
    "])\n",
    "fit_predict(pipeline, *train_test_data)"
   ],
   "id": "4f354986cce7a912",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features', \n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)), \n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'features__title__max_df': [0.05, 0.04, 0.06, 0.08, 0.1],\n",
    "    'features__title__min_df': [3,  2, 4, 5],\n",
    "    'features__title__ngram_range': [(0, 3), (0, 2), (0, 4), (0, 5)],\n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(train_test_data[0], train_test_data[1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "203f376843508ba8",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Здесь я решил дополнительно очистить все строки от символов юникода",
   "id": "72bac33497d5f2b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:08:50.449690Z",
     "start_time": "2024-04-21T09:08:49.865973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_unicode(df, column):\n",
    "    df[column] = df[column].apply(lambda x: ''.join(i for i in x if ord(i)<128))\n",
    "    return df\n",
    "\n",
    "train_df_new = remove_unicode(train_test_data[0], 'title')\n",
    "val_df_new = remove_unicode(train_test_data[2], 'title')"
   ],
   "id": "e403bb20236b80f",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T09:08:50.923134Z",
     "start_time": "2024-04-21T09:08:50.908135Z"
    }
   },
   "cell_type": "code",
   "source": "new_train_test_data = [train_df_new, train_test_data[1], val_df_new, train_test_data[3]]",
   "id": "f9a9b4fdf62e3e45",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\n",
    "        'features',\n",
    "        ColumnTransformer([\n",
    "            (\n",
    "                'url',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'url'\n",
    "            ),\n",
    "            (\n",
    "                'title',\n",
    "                TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)),\n",
    "                'title'\n",
    "            )\n",
    "        ])\n",
    "    ),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'features__title__max_df': [0.05, 0.04, 0.06, 0.08, 0.1],\n",
    "    'features__title__min_df': [3,  2, 4, 5],\n",
    "    'features__title__ngram_range': [(0, 3), (0, 2), (0, 4), (0, 5)],\n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=parameters, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(new_train_test_data[0], new_train_test_data[1])\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучший оценщик:\", grid_search.best_estimator_)"
   ],
   "id": "e6e042492345ed2e",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "the_best = Pipeline(steps=[('features',\n",
    "                 ColumnTransformer(transformers=[('url',\n",
    "                                                  TfidfVectorizer(max_df=0.05,\n",
    "                                                                  min_df=3,\n",
    "                                                                  ngram_range=(0,\n",
    "                                                                               3)),\n",
    "                                                  'url'),\n",
    "                                                 ('title',\n",
    "                                                  TfidfVectorizer(max_df=0.06,\n",
    "                                                                  min_df=2,\n",
    "                                                                  ngram_range=(0,\n",
    "                                                                               2)),\n",
    "                                                  'title')])),\n",
    "                ('clf', MultinomialNB(alpha=1))])\n",
    "fit_predict(the_best, *train_test_data)\n"
   ],
   "id": "442e311a60ad1e4",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "the_best = Pipeline(steps=[('features',\n",
    "                 ColumnTransformer(transformers=[('url',\n",
    "                                                  TfidfVectorizer(max_df=0.05,\n",
    "                                                                  min_df=3,\n",
    "                                                                  ngram_range=(0,\n",
    "                                                                               3)),\n",
    "                                                  'url'),\n",
    "                                                 ('title',\n",
    "                                                  TfidfVectorizer(max_df=0.06,\n",
    "                                                                  min_df=2,\n",
    "                                                                  ngram_range=(0,\n",
    "                                                                               2)),\n",
    "                                                  'title')])),\n",
    "                ('clf', MultinomialNB(alpha=1))])\n",
    "# fit_predict(the_best, *new_train_test_data)\n"
   ],
   "id": "34b0b8d4257bafc3",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "В этом месте я подумал, что зря разделил датасет перед обработкой и решил его объединить",
   "id": "b778956f71e84638"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:41:00.243844Z",
     "start_time": "2024-04-21T10:41:00.229844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "all_data = pd.concat([new_train_test_data[0], new_train_test_data[2]])\n",
    "all_y = np.concatenate([new_train_test_data[1], new_train_test_data[3]])\n",
    "\n",
    "labels_df = pd.DataFrame(all_y, columns=['label'])\n",
    "all_data.reset_index(drop=True, inplace=True)\n",
    "all_data_frame = pd.concat([all_data, labels_df],axis=1)\n",
    "all_data_frame.shape"
   ],
   "id": "6eea233641e62a8e",
   "execution_count": 67,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Мне пришла в голову мысль, добавить ещё два признака, а именно выделись самые частовстречающиеся слова для каждого их классов и добавлять 1 если в строке слова есть и 0 если нету.",
   "id": "8dda52373a44e96d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:06:30.872152Z",
     "start_time": "2024-04-21T10:06:30.460165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "class_1_titles = all_data_frame[all_data_frame['label'] == 0]['title']\n",
    "\n",
    "word_counts = Counter()\n",
    "for title in class_1_titles:\n",
    "    words = re.findall(r'\\b\\w+\\b', title)\n",
    "    word_counts.update(words)\n",
    "\n",
    "most_common_words = word_counts.most_common()\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "words_over_170 = [word for word, count in most_common_words if count > 170 and not is_number(word)]\n",
    "\n",
    "print(words_over_170)"
   ],
   "id": "f0fdda53702c4d72",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:06:32.640663Z",
     "start_time": "2024-04-21T10:06:32.631153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def words_presence_feature(df, words):\n",
    "    return df.apply(lambda x: 1 if any(word in x for word in words) else 0).to_frame()\n",
    "\n",
    "words_to_check = ['porn', 'sex', 'fuck', 'dick', 'pussy', 'sperm', 'webcam', 'boobs',]\n",
    "\n",
    "# words_to_check = ['porn', 'sex', 'xxx', 'girls', 'big', 'anal', 'naked', 'pussy', 'ass', 'biqle', 'tits', 'fucked', 'daftsex', 'blowjob', 'sexy', 'erotic', 'nude', 'dick', 'porno', 'fuck', 'fucks', 'fucking', 'erotica', 'milf', 'ancensored', 'cum', 'amateur', 'hardcore', 'adult', 'busty', 'lesbian', 'cock', 'homemade', 'xvideos', 'stockings', 'gay', 'chick', 'lesbians', 'boobs', 'masturbation', 'group']\n",
    "for_1_class = FunctionTransformer(words_presence_feature, validate=False, kw_args={'words': words_to_check})\n",
    "for_0_class = FunctionTransformer(words_presence_feature, validate=False, kw_args={'words': words_over_170})\n"
   ],
   "id": "493374c1717dcc78",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "the_best_1 = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2)), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', MultinomialNB(alpha=1))\n",
    "])\n",
    "# fit_predict(the_best_1, *new_train_test_data)"
   ],
   "id": "64093aa2edb7771c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Эксперементирую с Word2Vec (неудачно)",
   "id": "bb13c78884a99025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = [doc.split() for doc in X]\n",
    "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([abs(np.mean([self.model.wv[w] for w in doc.split() if w in self.model.wv]\n",
    "                                 or [np.zeros(self.vector_size)], axis=0))\n",
    "                         for doc in X])\n",
    "\n"
   ],
   "id": "ef025a71379bcacb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "\n",
    "the_best_1 = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3)), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2)), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "fit_predict(the_best_1, *new_train_test_data)"
   ],
   "id": "e2b1bb674d620c15",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Эксперементирую с токенизацией (чуть лучше)",
   "id": "f0ecab96b8eea0de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:07:30.694078Z",
     "start_time": "2024-04-21T10:07:30.690079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = nltk.ngrams(text, n)\n",
    "    return [''.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    # words = get_ngrams(text, 7)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return words\n",
    "# На данный момент даёт лучший результат\n",
    "the_best_1 = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3), tokenizer=preprocess_text), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2), tokenizer=preprocess_text), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "# fit_predict(the_best_1, *new_train_test_data)\n"
   ],
   "id": "857f675ed7392cfc",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 134,
   "source": "the_best_1.fit(all_data, all_y)",
   "id": "2ccf5d1db4cf0706",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "После получения лучшего результата решил поэксперементировать с дальнейшей очисткой датасета",
   "id": "7c52d48f97066471"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Решил посмотреть на распределение длин слов для каждого из классов",
   "id": "d6896675ead4ab80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T21:11:00.088184Z",
     "start_time": "2024-04-21T21:10:59.614136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "statistic_df = all_data_frame.copy()\n",
    "\n",
    "statistic_df['word_count'] = statistic_df['title'].apply(lambda x: len(x.split()))\n",
    "\n",
    "for class_name in all_data_frame['label'].unique():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    statistic_df[statistic_df['label'] == class_name]['word_count'].hist(bins=50)\n",
    "    plt.title(f'Распределение длин слов для класса {class_name}')\n",
    "    plt.xlabel('Количество слов')\n",
    "    plt.ylabel('Количество строк')\n",
    "\n",
    "    plt.xticks(np.arange(min(statistic_df['word_count']), max(statistic_df['word_count'])+1, 2.0))\n",
    "\n",
    "    plt.show()\n"
   ],
   "id": "f21aed0bb1c0c9a0",
   "execution_count": 83,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Пробую отбросить все значения минимальной длины",
   "id": "84d1238e9a5d0c34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "def check_total_length(text):\n",
    "    return 4 < len(text.split())\n",
    "\n",
    "\n",
    "df = all_data_frame[all_data_frame['title'].apply(check_total_length)]\n",
    "\n",
    "df.head()"
   ],
   "id": "8959f17c61c33056",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Заново делю на обучающую и тестовую выборку",
   "id": "acf457bff81f2392"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a490d7a1caf4f6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "print(\"Размер обучающего набора данных: \", train_df.shape)\n",
    "print(\"Размер тестового набора данных: \", test_df.shape)"
   ],
   "id": "6033ff6c5fb5a153",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 70,
   "source": "cutted_data = [train_df[['url', 'title']], train_df['label'], test_df[['url', 'title']], test_df['label']]",
   "id": "e271516b10eccafa",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "the_best_2 = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3), tokenizer=preprocess_text), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2), tokenizer=preprocess_text), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "fit_predict(the_best_2, *cutted_data)"
   ],
   "id": "21634b1cf05aff79",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T10:45:27.303410Z",
     "start_time": "2024-04-21T10:44:43.671007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "the_best_3 = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3), tokenizer=preprocess_text), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2), tokenizer=preprocess_text), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "the_best_3.fit(df[['url', 'title']], df['label'])# fit_predict(the_best_2, *cutted_data)"
   ],
   "id": "a982134bb2955117",
   "execution_count": 72,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T11:15:25.894038Z",
     "start_time": "2024-04-21T11:15:25.872037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def check_len(df):\n",
    "    return df.apply(lambda x: 0 if 5 <= len(x.split()) <= 8 else 1).to_frame()\n",
    "\n",
    "\n",
    "length = FunctionTransformer(check_len, validate=False)\n",
    "\n",
    "with_len = Pipeline(steps=[\n",
    "    ('features', ColumnTransformer(transformers=[\n",
    "        ('url', TfidfVectorizer(max_df=0.05, min_df=3, ngram_range=(0, 3), tokenizer=preprocess_text), 'url'),\n",
    "        ('title', TfidfVectorizer(max_df=0.06, min_df=2, ngram_range=(0, 2), tokenizer=preprocess_text), 'title'),\n",
    "        ('for_1_class', for_1_class, 'title'),\n",
    "        ('for_0_class', for_0_class, 'title')\n",
    "    ])),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "fit_predict(with_len, *new_train_test_data)"
   ],
   "id": "7f587c0465b9258",
   "execution_count": 82,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T11:11:39.052674Z",
     "start_time": "2024-04-21T11:11:39.050673Z"
    }
   },
   "cell_type": "code",
   "source": "BEST_MODEL = the_best_1",
   "id": "67d60c4f5d4bb9b",
   "execution_count": 79,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T11:11:39.637333Z",
     "start_time": "2024-04-21T11:11:39.587334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('VAL_DATA.pickle', 'rb') as file:\n",
    "    VAL_DATA = pickle.load(file)"
   ],
   "id": "33d57c9243699ac0",
   "execution_count": 80,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T11:12:44.536370Z",
     "start_time": "2024-04-21T11:11:40.064935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = BEST_MODEL.predict(VAL_DATA)\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df[\"label\"] = results\n",
    "\n",
    "test_df[[\"ID\", \"label\"]].to_csv(\"Current_best.csv\", index=False)"
   ],
   "id": "869b6a0cf85125b4",
   "execution_count": 81,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "К сожалению высокого скора добиться не удалось. Идеи с фильтрами текста слегка улучшили точность, однако по сравнению с лучшим моим решением этого недостаточно. Поэтому оставляю предыдущий. Максимальный скор на платформе составляет 0.9692169510900629\n",
    "Хотел бы попросить обратную связь по данному заданию -- что я мог бы улучшить, чего не заметил, и какие методы мог бы применить.\n",
    "Если это возможно, буду очень признателен."
   ],
   "id": "834d7dd9f5af92b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "aa8da52dd23e16a4",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
