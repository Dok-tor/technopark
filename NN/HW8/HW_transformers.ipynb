{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:05.125787Z",
     "start_time": "2024-11-21T10:25:05.119443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/character-tokenizer\")\n",
    "from charactertokenizer import CharacterTokenizer\n",
    "\n",
    "chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n",
    "model_max_length = 64\n",
    "tokenizer = CharacterTokenizer(chars, model_max_length)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "example = \"Привет\"\n",
    "tokens = tokenizer(example)\n",
    "print(tokens)"
   ],
   "metadata": {
    "id": "I5FSPMOSncpI",
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:05.435296Z",
     "start_time": "2024-11-21T10:25:05.431470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 39, 42, 26, 12, 18, 46, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задание: обучите модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers. Датасет для обучения можно взять отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n",
    "\n",
    "1. Напишите класс для Dataset/Dataloder и разбейте данные на случайные train / test сплиты в соотношении 50:50. (1 балл)\n",
    "2. Попробуйте обучить одну или несколько из моделей: Bert, Albert, Deberta. Посчитайте метрику Accuracy на train и test. (1 балл). При преодолении порога в Accuracy на test 0.8: (+1 балл), 0.85: (+2 балла), 0.89: (+3 балла).\n",
    "Пример конфигурации для deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json"
   ],
   "metadata": {
    "id": "KQkp36rEoScR"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Загрузим датасет и посмотрим на него"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:08.595863Z",
     "start_time": "2024-11-21T10:25:06.701803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('all_accents.tsv', sep='\\t', header=None, names=['word', 'stressed_word'])"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:11.163106Z",
     "start_time": "2024-11-21T10:25:11.158136Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      word stressed_word\n",
       "0      -де          -д^е\n",
       "1      -ка          -к^а\n",
       "2    -либо        -л^ибо\n",
       "3  -нибудь      -ниб^удь\n",
       "4       -с            -с"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>stressed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-де</td>\n",
       "      <td>-д^е</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-ка</td>\n",
       "      <td>-к^а</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-либо</td>\n",
       "      <td>-л^ибо</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-нибудь</td>\n",
       "      <td>-ниб^удь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-с</td>\n",
       "      <td>-с</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Сделаем предобработку"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:12.382381Z",
     "start_time": "2024-11-21T10:25:12.379156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_stress_position(word, stressed_word):\n",
    "    index = stressed_word.find('^')\n",
    "    if index == -1:\n",
    "        return None  # На случай \"приколов\" в датасете\n",
    "    # Убираем символ '^' из слова с ударением\n",
    "    stressed_word_clean = stressed_word.replace('^', '')\n",
    "    assert word == stressed_word_clean, f\"Words do not match: {word} != {stressed_word_clean}\"\n",
    "    return index  # Позиция ударной буквы в слове"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:18.036807Z",
     "start_time": "2024-11-21T10:25:12.771285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data['stress_position'] = data.apply(lambda row: get_stress_position(row['word'], row['stressed_word']), axis=1)\n",
    "data = data.dropna(subset=['stress_position'])\n",
    "data['stress_position'] = data['stress_position'].astype(int)\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:18.272903Z",
     "start_time": "2024-11-21T10:25:18.036807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.5, random_state=42)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Формируем класс датасета"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:18.277598Z",
     "start_time": "2024-11-21T10:25:18.273907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class StressDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.words = data['word'].tolist()\n",
    "        self.stress_positions = data['stress_position'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        stress_pos = self.stress_positions[idx]\n",
    "\n",
    "        encoding = self.tokenizer(word, return_tensors='pt', padding='max_length', truncation=True)\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        # Создаем метки для каждого символа\n",
    "        labels = torch.full(input_ids.shape, -100)  # Игнорируем позиции специальных токенов\n",
    "        # Позиция символа смещена на 1 из-за [CLS] токена\n",
    "        labels[stress_pos + 1] = 1  # Ударный символ\n",
    "        # Остальные символы\n",
    "        for i in range(1, len(word) + 1):\n",
    "            if labels[i] != 1:\n",
    "                labels[i] = 0  # Неударные символы\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Выбор модели  \n",
    "Взял Deberta (\"а что, я пальцем деланный что-ли - конечно осилю на своём железе самую тяжёлую модель...\")\n",
    "\n",
    "PS. Осилил, но не до конца"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:18.284156Z",
     "start_time": "2024-11-21T10:25:18.277598Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import DebertaForTokenClassification, Trainer, TrainingArguments",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Токенизатор возьму предложенный в условии задачи"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:20.373377Z",
     "start_time": "2024-11-21T10:25:19.466347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = CharacterTokenizer(chars, model_max_length)\n",
    "\n",
    "# Возьмём предобученную модель для классификации токенов\n",
    "model = DebertaForTokenClassification.from_pretrained('microsoft/deberta-base', num_labels=2)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:21.225189Z",
     "start_time": "2024-11-21T10:25:21.185440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = StressDataset(train_data, tokenizer)\n",
    "test_dataset = StressDataset(test_data, tokenizer)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Функция для подсчёта accuracy"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:22.432417Z",
     "start_time": "2024-11-21T10:25:22.429133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    mask = labels_flat != -100\n",
    "    pred_flat = pred_flat[mask]\n",
    "    labels_flat = labels_flat[mask]\n",
    "\n",
    "    return {'accuracy': accuracy_score(labels_flat, pred_flat)}"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Воспользуемся механизмом обучения, который предлагает библиотека Transformers"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:23.848329Z",
     "start_time": "2024-11-21T10:25:23.782984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacgr\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T10:25:26.152612Z",
     "start_time": "2024-11-21T10:25:25.906002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS):\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m--> 165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\transformers\\trainer.py:424\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    422\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_func \u001B[38;5;241m=\u001B[39m compute_loss_func\n\u001B[0;32m    423\u001B[0m \u001B[38;5;66;03m# Seed must be set before instantiating the model when using model\u001B[39;00m\n\u001B[1;32m--> 424\u001B[0m enable_full_determinism(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mseed) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mfull_determinism \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mset_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    426\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhp_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeepspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\transformers\\trainer_utils.py:105\u001B[0m, in \u001B[0;36mset_seed\u001B[1;34m(seed, deterministic)\u001B[0m\n\u001B[0;32m    103\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(seed)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n\u001B[1;32m--> 105\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    106\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mmanual_seed_all(seed)\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\torch\\_compile.py:31\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     28\u001B[0m     disable_fn \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mdisable(fn, recursive)\n\u001B[0;32m     29\u001B[0m     fn\u001B[38;5;241m.\u001B[39m__dynamo_disable \u001B[38;5;241m=\u001B[39m disable_fn\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdisable_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600\u001B[0m, in \u001B[0;36mDisableContext.__call__.<locals>._fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    598\u001B[0m prior \u001B[38;5;241m=\u001B[39m set_eval_frame(callback)\n\u001B[0;32m    599\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 600\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    602\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\torch\\random.py:46\u001B[0m, in \u001B[0;36mmanual_seed\u001B[1;34m(seed)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcuda\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39m_is_in_bad_fork():\n\u001B[1;32m---> 46\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmps\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mmps\u001B[38;5;241m.\u001B[39m_is_in_bad_fork():\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\torch\\cuda\\random.py:127\u001B[0m, in \u001B[0;36mmanual_seed_all\u001B[1;34m(seed)\u001B[0m\n\u001B[0;32m    124\u001B[0m         default_generator \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdefault_generators[i]\n\u001B[0;32m    125\u001B[0m         default_generator\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n\u001B[1;32m--> 127\u001B[0m \u001B[43m_lazy_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\torch\\cuda\\__init__.py:244\u001B[0m, in \u001B[0;36m_lazy_call\u001B[1;34m(callable, **kwargs)\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_lazy_call\u001B[39m(\u001B[38;5;28mcallable\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 244\u001B[0m         \u001B[38;5;28;43mcallable\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    246\u001B[0m         \u001B[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001B[39;00m\n\u001B[0;32m    247\u001B[0m         \u001B[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001B[39;00m\n\u001B[0;32m    248\u001B[0m         \u001B[38;5;66;03m# else here if this ends up being important.\u001B[39;00m\n\u001B[0;32m    249\u001B[0m         \u001B[38;5;28;01mglobal\u001B[39;00m _lazy_seed_tracker\n",
      "File \u001B[1;32m~\\Documents\\GitReps\\technopark\\NN\\.venv_nn\\Lib\\site-packages\\torch\\cuda\\random.py:125\u001B[0m, in \u001B[0;36mmanual_seed_all.<locals>.cb\u001B[1;34m()\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(device_count()):\n\u001B[0;32m    124\u001B[0m     default_generator \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdefault_generators[i]\n\u001B[1;32m--> 125\u001B[0m     \u001B[43mdefault_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Здесь мне резко стало \"больно\", так как мало того, что обучалась модель почти 31 час, так ещё и завершилось обучение с ошибкой из-за нехватки памяти. Я связываю это с тем, что выбрав Deberta я \"выстрелил себе в ногу\". Возможно обычная BERT справилась бы быстрее.  \n",
    "Однако, как видно из статистики обучения, точность после первой эпохи составила 0.98. А это выше, чем необходимая точность на полный балл. Так как условия задачи выполнены, то я не рискнул повторять цикл обучения с другой моделью, так как у меня не хватило бы времени."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### К сожалению при вылете в ноутбуке не сохранилось значений результатов обучения, однако все они есть в /logs tensorboard-а.  \n",
    "### Здесь приведу скрины оттуда. Как видно, точноть на тестовой выборке составила 0,9849\n",
    "Для eval:   \n",
    "<img src=\"pictures/eval_accuracy.png\" alt=\"Alt text\">  \n",
    "\n",
    "Для train:  \n",
    "<img alt=\"Alt text\" src=\"pictures/train_accuracy.png\"/>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
